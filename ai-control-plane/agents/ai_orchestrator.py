"""
AZALS - AI Self-Healing Orchestrator
=====================================

AVERTISSEMENT S√âCURIT√â:
Ce module a √©t√© s√©curis√© mais l'ex√©cution automatique de code g√©n√©r√© par IA
reste intrins√®quement risqu√©e. En production, d√©sactivez AUTO_APPLY_FIXES.

Modes de fonctionnement:
- MONITORING_ONLY: D√©tecte et signale les erreurs (recommand√© en production)
- SUGGEST_FIXES: G√©n√®re des suggestions de fix mais ne les applique pas
- AUTO_APPLY_FIXES: DANGEREUX - Applique automatiquement les fixes (dev only)
"""

import os
import re
import time
import subprocess
import json
import logging
from pathlib import Path
from typing import Optional, Tuple

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI non disponible")

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic non disponible")

print("ü§ñ AI Self-Healing Orchestrator is running")

# Configuration s√©curis√©e
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_KEY = os.getenv("ANTHROPIC_API_KEY")

# MODE DE FONCTIONNEMENT (S√âCURIT√â)
# "monitoring" = d√©tection seulement
# "suggest" = g√©n√®re des suggestions
# "auto" = applique automatiquement (DANGEREUX)
ORCHESTRATOR_MODE = os.getenv("AI_ORCHESTRATOR_MODE", "monitoring")

# R√©pertoire de base pour les prompts (protection path traversal)
PROMPTS_BASE_DIR = Path(__file__).parent / "prompts"

# Commandes autoris√©es (whitelist)
ALLOWED_COMMANDS = {
    "docker_logs_api": ["docker", "logs", "api", "--tail", "100"],
    "docker_logs_frontend": ["docker", "logs", "azals_frontend", "--tail", "50"],
    "docker_logs_nginx": ["docker", "logs", "azals_nginx", "--tail", "50"],
    "git_diff": ["git", "diff", "HEAD~1"],
    "git_status": ["git", "status"],
}

# Initialisation des clients (si disponibles)
openai_client = None
anthropic_client = None

if OPENAI_AVAILABLE and OPENAI_KEY:
    openai_client = OpenAI(api_key=OPENAI_KEY)

if ANTHROPIC_AVAILABLE and ANTHROPIC_KEY:
    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_KEY)


def read_prompt(name: str) -> str:
    """
    Lit un fichier prompt de mani√®re s√©curis√©e.

    S√âCURIT√â: Protection contre path traversal.
    """
    # Valider le nom (alphanumeric et underscores seulement)
    if not re.match(r'^[a-zA-Z0-9_]+$', name):
        raise ValueError(f"Nom de prompt invalide: {name}")

    # Construire le chemin s√©curis√©
    prompt_path = (PROMPTS_BASE_DIR / f"{name}.prompt").resolve()

    # V√©rifier que le chemin est bien dans le r√©pertoire autoris√©
    try:
        prompt_path.relative_to(PROMPTS_BASE_DIR.resolve())
    except ValueError:
        raise ValueError(f"Tentative d'acc√®s hors du r√©pertoire prompts: {name}")

    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt non trouv√©: {name}")

    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()


def run_safe_command(command_key: str) -> Tuple[bool, str]:
    """
    Ex√©cute une commande de la whitelist de mani√®re s√©curis√©e.

    S√âCURIT√â: Seules les commandes pr√©d√©finies peuvent √™tre ex√©cut√©es.
    """
    if command_key not in ALLOWED_COMMANDS:
        logger.error(f"Commande non autoris√©e: {command_key}")
        return False, f"Commande non autoris√©e: {command_key}"

    cmd = ALLOWED_COMMANDS[command_key]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60,
            shell=False  # S√âCURIT√â: Pas de shell
        )
        output = result.stdout + result.stderr
        return True, output[:10000]  # Limite de taille
    except subprocess.TimeoutExpired:
        return False, "Timeout d√©pass√©"
    except Exception as e:
        return False, f"Erreur: {str(e)}"

def get_logs() -> str:
    """R√©cup√®re les logs des conteneurs de mani√®re s√©curis√©e."""
    logs_parts = []

    success, api_logs = run_safe_command("docker_logs_api")
    if success:
        logs_parts.append(f"=== API LOGS ===\n{api_logs}")

    success, frontend_logs = run_safe_command("docker_logs_frontend")
    if success:
        logs_parts.append(f"=== FRONTEND LOGS ===\n{frontend_logs}")

    success, nginx_logs = run_safe_command("docker_logs_nginx")
    if success:
        logs_parts.append(f"=== NGINX LOGS ===\n{nginx_logs}")

    return "\n\n".join(logs_parts)


def get_diff() -> str:
    """R√©cup√®re le diff git de mani√®re s√©curis√©e."""
    success, diff = run_safe_command("git_diff")
    return diff if success else ""


def call_openai(prompt: str, data: str) -> Optional[str]:
    """Appelle OpenAI de mani√®re s√©curis√©e."""
    if not openai_client:
        logger.warning("OpenAI non configur√©")
        return None

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": prompt[:4000]},  # Limite
                {"role": "user", "content": data[:8000]}  # Limite
            ],
            max_tokens=2000
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Erreur OpenAI: {e}")
        return None


def call_claude(prompt: str, data: str) -> Optional[str]:
    """Appelle Claude de mani√®re s√©curis√©e."""
    if not anthropic_client:
        logger.warning("Anthropic non configur√©")
        return None

    try:
        msg = anthropic_client.messages.create(
            model="claude-3-5-sonnet-latest",
            max_tokens=2000,
            system=prompt[:4000],  # Limite
            messages=[{"role": "user", "content": data[:8000]}]  # Limite
        )
        return msg.content[0].text
    except Exception as e:
        logger.error(f"Erreur Claude: {e}")
        return None


def analyze_errors(logs: str) -> dict:
    """Analyse les logs pour d√©tecter les erreurs."""
    error_patterns = [
        "ERROR", "Exception", "Traceback", "CRITICAL",
        "500 Internal Server Error", "502 Bad Gateway"
    ]

    # Filtrer les faux positifs
    ignore_patterns = ["403 Forbidden"]  # Peut √™tre normal

    errors_found = []
    for pattern in error_patterns:
        if pattern in logs:
            # V√©rifier que ce n'est pas un faux positif
            if not any(ignore in logs for ignore in ignore_patterns):
                errors_found.append(pattern)

    return {
        "has_errors": len(errors_found) > 0,
        "patterns": errors_found,
        "log_length": len(logs)
    }


def save_suggestion(suggestion: str, suggestion_type: str) -> str:
    """
    Sauvegarde une suggestion de fix pour revue manuelle.

    S√âCURIT√â: Les suggestions sont sauvegard√©es mais JAMAIS ex√©cut√©es automatiquement.
    """
    suggestions_dir = Path(__file__).parent / "suggestions"
    suggestions_dir.mkdir(exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"{suggestion_type}_{timestamp}.txt"
    filepath = suggestions_dir / filename

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# Suggestion g√©n√©r√©e le {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Type: {suggestion_type}\n")
        f.write(f"# IMPORTANT: Revue manuelle requise avant application\n\n")
        f.write(suggestion)

    return str(filepath)


def main_loop():
    """Boucle principale s√©curis√©e."""
    logger.info(f"Mode: {ORCHESTRATOR_MODE}")

    if ORCHESTRATOR_MODE == "auto":
        logger.warning("‚ö†Ô∏è MODE AUTO ACTIV√â - D√©conseill√© en production!")

    while True:
        try:
            logger.info("V√©rification des logs...")
            logs = get_logs()
            logger.info(f"R√©cup√©r√© {len(logs)} caract√®res de logs")

            analysis = analyze_errors(logs)

            if analysis["has_errors"]:
                logger.warning(f"‚ö†Ô∏è Erreurs d√©tect√©es: {analysis['patterns']}")

                if ORCHESTRATOR_MODE == "monitoring":
                    # Mode monitoring: on signale seulement
                    logger.info("Mode monitoring - Erreurs signal√©es, pas d'action")

                elif ORCHESTRATOR_MODE in ("suggest", "auto"):
                    # G√©n√©rer une analyse
                    try:
                        debug_prompt = read_prompt("debug")
                        debug_analysis = call_openai(debug_prompt, logs)

                        if debug_analysis:
                            logger.info("Analyse de debug g√©n√©r√©e")

                            if ORCHESTRATOR_MODE == "suggest":
                                # Sauvegarder pour revue manuelle
                                fix_prompt = read_prompt("fix")
                                fix_suggestion = call_claude(fix_prompt, debug_analysis)

                                if fix_suggestion:
                                    filepath = save_suggestion(fix_suggestion, "fix")
                                    logger.info(f"‚úÖ Suggestion sauvegard√©e: {filepath}")
                                    logger.info("‚ö†Ô∏è REVUE MANUELLE REQUISE avant application")

                            elif ORCHESTRATOR_MODE == "auto":
                                # MODE DANGEREUX - D√©sactiv√© par s√©curit√©
                                logger.error("‚ùå Mode auto d√©sactiv√© pour raisons de s√©curit√©")
                                logger.error("L'ex√©cution automatique de code IA n'est pas autoris√©e")
                                # NE PAS ex√©cuter de code g√©n√©r√© par l'IA automatiquement

                    except FileNotFoundError as e:
                        logger.error(f"Fichier prompt non trouv√©: {e}")
                    except Exception as e:
                        logger.error(f"Erreur lors de l'analyse: {e}")

            else:
                logger.info("‚úÖ Aucune erreur d√©tect√©e")

        except Exception as e:
            logger.error(f"‚ùå Erreur dans la boucle principale: {e}")

        time.sleep(30)


if __name__ == "__main__":
    main_loop()
